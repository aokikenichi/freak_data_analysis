# ヤバいデータ分析（仮称）

# まえがき
　データ分析はなんて広いんだろう。影響力の強まりに応じ、自然・社会・人間ほぼすべてが対象となりどんどん拡大していく。対象に応じ手法も広がり複雑化し、学ぶべきことが多すぎる。データサイエンティスト協会のスキルチェックリストVer.3.00[^1]も500超の項目があります。読むべき図書も良書と思われるものだけでも増え続けており、もう手に負えない状況です。
ただ、これはやってはだめだ、ここを知らないと道に迷う、という絶対に知っておくべき点は学べる範囲だと思います。本書では、データ分析において間違えやすい、誤解しやすい点を共有し、データ分析全体をよくする目的で、かつ
- データ分析の技術書・専門書に分野ごとには書かれてはいるが1つにまとまっておらず目に触れにくいもの
- データ分析の技術書・専門書でもスルーされていたらい場合によっては誤っていると思われるもの

で自分なりに取り上げまとめました。
　本書でやや批判的に取り上げた方や書籍につきまして批判することが本意ではありません。取り上げた方や書籍は指摘事項以外は有用な方、書籍と判断したもののみ挙げております。
書名は、好著『ヤバい経済学』『ヤバい社会学』『ヤバい統計学』『ヤバい経営学』に憧れてつけました。「ヤバい」シリーズとテイストが変わっており質は低いですが、皆様の分析の一助となれば幸いです。

[^1]:https://www.datascientist.or.jp/common/docs/skillcheck_ver3.00.pdf

# I部：知らないとまずい
## 1. 人工知能と機械学習と統計学と
　今世の中で「AI」あるいは「人工知能」と言った場合に多くは機械学習のこと特に深層学習のことを指すことが多いと思われます。ただ、「ヨーロッパのAIスタートアップ40％がAIを使っていない…… 大事なのは定義か？」[^2]などという記事もあり、実際はITで何かやってるだけのことも多い。人により定義がバラバラでしかも人工知能、機械学習、統計学と似たような似てないような言葉が乱立しています。
　人工知能学会がまとめた「AI研究初学者と異分野研究者のためのAI研究の俯瞰図　AIマップ」[^3]というものがある。これを見るとAI研究といっても実に広いことがわかる。機械学習やその中の深層学習などはごくごく一部でしかない。この中では統計学はAI研究を支える技術ということになっている。
　例えば重回帰分析はAIなのか機械学習なのか統計学なのか。

　渋谷駅前で働くデータサイエンティストのブログの「『統計学と機械学習の違い』はどう論じたら良いのか」[^4]が私としてはしっくり来ている。

- 統計学はデータを「説明」することにより重きを置く
- 機械学習はデータから「予測」することにより重きを置く
- とは言え、統計学と機械学習の違いは基本的にはそれほど大きくないし互いに重なる部分だらけ

　なので重回帰分析は統計学の本にも出ていますし、機械学習の本にも出ています。でもそれをAIと言い出すと何だか違うんじゃねという気がします。でも重回帰分析でことが済めば全く問題がない。

　学術研究ではなくビジネスへの適用ならばやはり、AIか否か、機械学習か否か、統計学か否かではなく、課題設定しそれが解決できるか否か、に尽きるのだと。

結論
- 与太話は話半分^10くらいで聞き流そう
- AIマップなど専門家の物差しを活用しよう
- 課題設定しそれが解決できるか否かに尽きる

[^2]:https://ledge.ai/ai-definition-problem/
[^3]:https://www.ai-gakkai.or.jp/resource/aimap/
[^4]:https://tjo.hatenablog.com/entry/2015/09/17/190000

## 2. 〇〇学習
　とはいえ、学んでいくために概念の交通整理が必要です。〇〇学習とつくものをざっと挙げると

　機械学習、教師あり学習、半教師あり学習、教師なし学習、強化学習、深層学習（ディープラーニング）、多様体学習、転移学習、オンライン学習、バッチ学習、表現学習などがある。これらの関係を見ていく。

　包含関係を描くと下図のようになる。

<img width="616" alt="スクリーンショット 2023-05-05 13 32 53" src="https://user-images.githubusercontent.com/17631286/236378303-b8c959e0-be2a-41a3-9b5a-4acd6d857acb.png">

　人工知能の中に機械学習があり、その内訳として教師あり学習、教師なし学習、強化学習。人工知能には機械学習の他にもルールベースや推論、その他の技術がある。

- 機械学習
  - 機械学習の性質による分類
    - 教師あり学習
      - 正解となるデータを与えて、できるだけそれに近づくように学習させること。分類（判別）や回帰等がある。
    - 教師なし学習
      - 正解データを与えず、データからその特徴を見出すような学習をすること。クラスタリングや次元削減等がある。
    - 半教師あり学習
      - 一部のデータに正解を与え、正解が無いデータも、正解があるデータとの関連性から学習していく手法のこと。
    - 強化学習
      - 正解データはないが、環境を学習するエージェントがあり、エージェントが行動することで報酬を得て、その報酬を最大化するように行動しながら学習する手法のこと。
    - 転移学習
      - 一度教師ありで学習済みのモデルを元に、新たなデータで学習すること。学習済みのデータと新たなデータは異なるのだが似ていれば学習が応用できるという手法。
  - 機械学習の実行の仕方による分類
    - バッチ学習
      - データをひとまとめに投入して学習すること。小分けにして学習するミニバッチ学習もある
    - オンライン学習
      - データが入ってくるたびにその都度そのデータだけで学習すること
    - c.f. 「オンライン学習とバッチ学習」[^5]
  - 機械学習の手法・アルゴリズムの1つ
    - 深層学習
      - 機械学習の手法の1つである、ニューラルネットワークの1つ。ニューラルネットワークは入力層、中間層、出力層からなるが、中間層が多い（深い）ものを特に深層学習と呼ぶ。教師あり学習、教師なし学習、強化学習にも用いられる。
    - 多様体学習
      - 機械学習の手法の1つ。多様体とは、局所的にはユークリッド平面と同様にみなせる構造のこと。主に次元削減として、多次元のデータを多様体を通して低次元に縮約する技術。主成分分析PCAもその1つ。「【多様体学習】LLEとちょっとT-SNE」[^6]
- 似た言葉
  - データマイニング
    - Wikipedia:機械学習「データマイニングとの関係」[^7]にあるが
      - 機械学習の目的は、訓練データから学んだ「既知」の特徴に基づく予測である。
      - データマイニングの目的は、それまで「未知」だったデータの特徴を発見することである。
    - ただ、機械学習でも未知の特徴を発見するし、データマイニングでも予測をする部分もあるので、両者はかなりオーバーラップしている。私見だが、データを機械で学習させていまおうと研究していたグループが「機械学習」という言葉を使い、データから意味を見出そうぜと研究していたグループが「データマイニング」という言葉を使い、あれ同じことだよね、となっているのが現在と思っている。
  - 多変量解析
    - 統計学の1つで、1つの変数で検定などを行うのに比べて、多数の説明変数を扱う解析のことを読んでいると思う。これもデータマイニングと同じくかなり機械学習とオーバーラップがある。
  - 統計モデル
    - 「『統計学と機械学習の違い』はどう論じたら良いのか」[^8]が詳しいが、
      - 統計学はデータを「説明」することにより重きを置く
      - 機械学習はデータから「予測」することにより重きを置く

[^5]:https://dev.classmethod.jp/articles/online-batch-learning/
[^6]:https://www.hellocybernetics.tech/entry/2017/07/06/133450
[^7]:https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92#%E3%83%87%E3%83%BC%E3%82%BF%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%A8%E3%81%AE%E9%96%A2%E4%BF%82
[^8]:https://tjo.hatenablog.com/entry/2015/09/17/190000

## 3. データ分析手法に迷う前にまず図を描け
　まぁみんな図を描かない。図解の技術みたいな本がバカ売れするということは普段描かないからだろう。Excelでだって描けるのに。機械学習の入門本でも作図についてはサラッと流されていることが多い。
　データジャーナリズムという言葉もあるらしい。今回のコロナショックでは図を取り入れるメディアも増えた。
　ただ、作図の仕方はチュートリアルなりマニュアルを読めば分かるが、そもそもどういう図だと受け手に伝わるかが難しい。箱ひげ図をよく描くがあまりユーザー側に響かない。2020年のセンター試験数学に箱ひげ図が出ていたのでこれからの若い人は箱ひげ図は当たり前になるかもしれないがしばらくは受け入れられにくいのかもしれない。作図例などで発想を広げていくしかないかなと思う。

　言語ごとに代表的な作図ライブラリがあり、作図例も豊富。
- Python
  - Matplotlib: Visualization with Python[^9]
  - seaborn: statistical data visualization[^10]
  - Plotly Python Open Source Graphing Library[^11]
- R
  - ggplot2[^12]
    - 『Data Visualization A practical introduction』Kieran Healy[^13]
      - ライブラリではなく社会科学系で好評な書籍らしい。出版もされているが同内容をWebで読める。
- JavaScript
  - D3 Data-Driven Documents[^14]
  - Graph Gallery
    - Pythonでの作図ギャラリー
      - https://python-graph-gallery.com/
    - Rでの作図ギャラリー
      - https://www.r-graph-gallery.com/
    - JavaScript(D3.js)での作図ギャラリー
      - https://www.d3-graph-gallery.com/
    - 作図したい用途毎にチャート式で作図方法が選べる
      - https://www.data-to-viz.com/
-　言語毎ではなく使用する側として
  - BBC Style[^15]
    - Rにて、BBCが取り入れている作図のノウハウが見られる。

[^9]:https://matplotlib.org/
[^10]:https://seaborn.pydata.org/
[^11]:https://plotly.com/python/
[^12]:https://ggplot2.tidyverse.org/
[^13]:https://socviz.co/
[^14]:https://d3js.org/
[^15]:https://bbc.github.io/rcookbook/

## 4. データ分析手法はたくさんあるが使うのは5種類のみ
　複数の機械学習手法を統一したインターフェースで利用できるRのライブラリの一つcaretには学習法の一覧があり、2019-03-27現在238個の手法がある[^16]。ここにあるのは機械学習のみであり、この他にも統計学的手法もある。時系列データ、自然言語データ等にはそれぞれ特化した手法があるが、汎用的なデータの場合にはこの中から適切な手法を選んでいくことになる。ただ、よく使われるのはごく少数である。
　第2回データサイエンティストオブザイヤーを受賞した渋谷直正氏によると「実務で使う分析手法は5つで十分、マーケッターこそデータサイエンティスト候補」[^17]とのことである。

　分析手法を使う前の前提として、グラフを描く、であり、5つの手法とは、　
- 「クロス集計」
- 「ロジスティック回帰分析」
- 「決定木分析」
- 「アソシエーション分析」
- 「クラスター分析（k-平均法）」
 
である。マーケティング寄りかつ判別のみだが至って正論だと思う。

また、農業系の分析者への多数の講師経験のある食品産業技術総合研究機構の三中信宏氏の著書『統計思考の世界』[^18]においても

>	「計算する前によく見よう」「いかに可視化するか」「統計計算やモデリングの前に、生の情報をいかにきちんと”見る”かは統計グラフィクスやインフォグラフィクスの最前線につながっていきます」「もっと図表を！」

と図を描くことの重要性を強調しておられます。
　
　まずこれらをしてから深い分析が必要、あるいは回帰の場合に別の手法を考えればよいと思います。
　そして補足。入門書によく出てくるサポートベクターマシンですが、理解が難しめな割には精度が出ず、ビジネス実務でもKaggleのようなコンペティションでもほとんど使われないかと思います（カーネル法の考え方はあちこちで使われ重要ですが入門において他の手法と同じ重要さで学ぶべきとは思えません。

　その他詳しくはQiita「氷解！データ分析、機械学習手法ってたくさんあるけどいつどれを使えばよいのか」[^19]に書いたので読んでいただきたい。

結論
- ビジネスデータ（判別が中心）の場合はまず下記5つに習熟のこと。
  - クロス集計
  - ロジスティック回帰分析
  - 決定木分析
  - アソシエーション分析
  - クラスター分析（k-平均法）
- 回帰を含みより深く分析する場合は下記5つに習熟のこと。
  - 説明性を重視する場合（青木注：最近では機械学習の説明・解釈性としてSHAPなどの手法が出てきており、重回帰や決定木にこだわる必要性は減ってきたかも）
    - 回帰
      - 重回帰分析
      - 回帰木分析
    - 判別
      - ロジスティック回帰分析
      - 決定木分析
    - 予測精度を重視する場合
      - 回帰、判別とも
        - アンサンブル学習（精度の良さと計算の高速性からlightgbm）
-　つまり、クロス集計、木、重回帰、ロジスティック回帰、アンサンブル学習、をきちんと学ぶことが重要かと。 

[^16]:https://topepo.github.io/caret/available-models.html
[^17]:https://lp.sendenkaigi.com/marketing
[^18]:三中信宏． 統計思考の世界． 技術評論社， 2018， 240p.
[^19]:https://qiita.com/aokikenichi/items/688e66d10a944051039c

## 5. 【要修正】データ分析に必要な数学は？ __★★★この項だけ長すぎるかまた２０２０年くらいの古い情報なので要整理★★★__

　センター試験数学Ⅰで箱ひげ図が出てきている。データ分析とデータ分析に数学を用いるのが一般的になってきているのだろう。
　データ分析には数学が必要だと人は言う。大学一年レベルの数学で十分だと言われたり、もっと必要だと言われたり。
　一般人のイメージする数学は
 
$$\int_a^b{xdx}$$

 みたいなのだろうけど数学って本当に広い（Masaki Koga 氏の「数学にはどんな研究分野がある？数学の世界地図を紹介！！」[^20]）。ならば数学全般ではなく、データサイエンス系の大学のカリキュラムを見ればいいではないかとのことで[^21]

|      | 滋賀大学[^22] | 横浜市立大学[^23] |東京都市大学[^24] |
| :--- | :---         | :---            | :---          |
| １年  | 解析学<br/>線形代数<br/>統計学要論 | 集合位相<br/>微分積分学<br/>線形代数<br/>統計の数理 |微分積分学<br/>線形代数学<br/>基礎確率統計<br/>数理統計 |
| 2年  | 統計数学<br/>応用数学[^25] | 情報理論<br/>代数学<br/>応用統計学 | 微分方程式論<br/>ベクトル解析学<br/>代数学<br/>フーリエ解析学<br/>関数論<br/>情報理論<br/>オペレーションズリサーチ |
| 3年  | 最適化理論<br/>確率論 | 最適化理論 | - |
| 4年  |情報理論<br/>統計学特論 | - | - |

 大学ではないが、東京大学エクステンション・データサイエンススクール・技術実務者コースでは
- 前提知識　高校までの理系数学と大学1・2年生の数学の一部（偏微分、積分、行列）
- 数学科目　統計学、最適化

ざっくりまとめると

- 高校1年[^26]
  - 数学Ⅰ
    - 数と式、図形と計量、二次関数、データの分析
  - 数学A
    - 図形の性質、場合の数と確率、数学と人間の活動
- 高校2年
  - 数学Ⅱ
    - いろいろな式、図形と方程式、いろいろか関数、微分・積分の考え
  - 数学B
    - 数列、統計的な推測
- 高校3年
  - 数学Ⅲ
    - 極限、微分法、積分法
  - 数学C
    - ベクトル、平面上の曲線と複素数平面、数学的な表現の工夫
      - 「数学的な表現の工夫」の1つに「行列」があるがデータ分析に重要な線形代数の基本はほとんどなく大学に入ってから
- 大学1年
  - 微分積分学
    - 解析学の一部なので「解析学」という科目の場合あり
    - 高校までの微分積分との大きな違いは、
      - 定義が厳密になる
      - 高校　1変数のみ
      - 大学　複数の変数を扱う　偏微分　重積分　などが登場する
  - 線形代数学
  - 統計学
    - 「確率統計学」という科目の場合あり。「確率論」はまた別の科目
  - （集合・位相）
    - 上位の数学をする場合には必須だが抽象的で難易度が高く、他の科目でさらっと入り口を眺めるだけの場合とほぼ学ばない場合もあり
- 大学2年以上
  - 応用統計学
    - 多変量解析など機械学習に近い科目
  - 最適化理論
    - 機械学習の理論の中核を担う科目
  - 情報理論
    - データの中にどれだけ有益な情報と、無益なノイズが入っているかなどを問う科目
  - 離散数学
    - 大学や教科書によってスコープが異なることが多いが、組合せ問題やグラフ理論などを扱う科目

　データ分析について数学以外も学んで、必要な数学について私がいろいろ学んだ結果として
- 「データサイエンス、データ分析、機械学習関連の本」[^27]
  - 推薦図書の一覧
- 「データ分析関連（データサイエンス、データ分析、機械学習）書籍マップ Ver.1.00」[^28]
  - 推薦図書の読み進める順番、前提知識、次に読む本の繋がり
- 「データサイエンティスト協会スキルチェックリストver.3.00『データサイエンティスト』に必要な本」[^29]
  - スキルチェックリストのスキルを学ぶために必要な本
- 数学は楽をするためにある－機械学習のための数学入門前の準備体操～その１数学記号、公理・定義・定理、微分積分～[^30]
  - 【要追記】
- データサイエンス、データ分析、機械学習に必要な数学[^31]
  - 【要追記】
データサイエンス、データ分析、機械学習に必要な数学２[^32]
  - 【要追記】

まとめみると
- 微分積分
  - 統計学はいろいろな分布を扱うが、分布は関数である。関数の特徴を知るためには、細かく分けて見る微分、そして細かく分けたものを1つに足し合わせてまとめる積分が必要となる。
  - 機械学習ではデータから学習モデルを作る。そしてその学習モデルとデータとの誤差を最小にする。学習モデルとは関数であり、データとの誤差も関数となる。関数を採用にするためには傾きの議論が必要であり、微分が必要となる。
- 線形代数
  - データはだいたいの場合において特徴を示す列とサンプルが1行となる行列の形であらわされ、行列を扱う線形代数が必要となる。
  - データの特徴をより少ないデータで表すために行列の性質を用いてデータを分解する必要場合に線形代数が必要となる。
- 統計学
  - 【要追記】
- 集合・位相
  - 【要追記】
- 最適化理論
  - 【要追記】 
- 微分幾何学、位相幾何学
  - データを形として捉えるほうが都合がいい場合があり、形を扱う幾何学が必要となる。
  - 多様体学習
    - 【要追記】
  - TDA
    - 【要追記】
- 関数解析
  - 【要追記】
- 代数学
  - 群論
    - 【要追記】

[^20]:https://note.com/masakikoga1/n/n3ce9436e0a1c
[^21]:武蔵野大学にもデータサイエンス学部があるがカリキュラム上で数学の科目を明示していないので略。数学と情報工学やデータ分析の境界線上の科目もある。また、データ分析系の科目の中で必要な数学を学ぶと言うのもありえるため、ざっくりとした分類で失礼。
[^22]:データサイエンス学部データサイエンス研究科 https://www.ds.shiga-u.ac.jp/about/ds/curriculum/
[^23]:データサイエンス学部データサイエンス学科 https://www.yokohama-cu.ac.jp/academics/ds/curriculum.html
[^24]:知識知能情報工学科 https://www.tcu.ac.jp/tcucms/wp-content/uploads/2019/06/curriculm2019-c-02.pdf
[^25]:離散数学、グラフ理論、待ち行列などらしい https://success.shiga-u.ac.jp/Portal/Public/Syllabus/DetailMain.aspx?lct_year=2020&lct_cd=7022100701
[^26]:数学 (教科) - Wikipedia 細かく変更があるが2022年からの新課程で例示
[^27]:https://qiita.com/aokikenichi/items/ae4df263f591e47528a6
[^28]:https://qiita.com/aokikenichi/items/6bbb4d2a00f979927be4
[^29]:https://qiita.com/aokikenichi/items/febcc7ae4f803050e90b
[^30]:https://qiita.com/aokikenichi/items/8fcf8b2401438af71cd2
[^31]:https://qiita.com/aokikenichi/items/4d683dc5774f844f8113
[^32]:https://qiita.com/aokikenichi/items/229f2886578f5eee4649

## 6. パラメータ、母数、サンプル数、サンプルサイズ、n数

> 男女10名ずつでは母数が少なく、男女50名ずつにサンプル数を増やした。

 というような、誤用が多い。
　
 母数はパラメーター(parameter)の訳語であり、分母やサンプルサイズの意味ではない。
 
 上記例では、サンプル数は2でサンプルサイズを10から50に増やした、となる。

 これは、データ分析に長けた方でもかなり誤用が多く、指摘しても理解されないかもしれない。ネットでも、データ分析の入社面接で母数の意味を聞いて分母やサンプルサイズと答えたら落とすというのが真偽はともかくネタとして言われるほど誤用に怒っている方もいる。また、n数はほぼサンプルサイズと同じ意味に使われるようだ。

 誤用自体はスルーしてもいいかなと個人的には思いますが、でも誤用している時点で元の概念を誤っている可能性があり、こういうところを大切にしていきたい。

c.f. コラム：サンプル数再考／労働政策研究・研修機構（JILPT）[^33]

[^33]:https://www.jil.go.jp/column/bn/colum0151.html


## 7. 2群検定：Wilcoxonの順位和検定を積極的に利用すればよいわけではない

 三重大学の奥村晴彦氏のTwitterでの指摘[^34]で知りましたが

> 最近買った本。こういう誤解が多いけれど，Wilcoxon順位和検定は等分散の場合のt検定みたいなもので，図(b)(c)に適用するものではないと思う

で、『新版　統計学のセンス』[^35]を読んでみますと、

（等分散でない2群の比較において）

> データの順位だけを利用するのでどんな分布型にも適用できるノンパラメトリック検定を使用する。
> （中略）
> 　t検定に対するノンパラメトリック検定としてWilcoxonの順位和検定が推薦できる。
> （中略）
> ノンパラメトリック検定
> Wilcoxonの順位和検定を積極的に利用しよう。

と等分散でない2群の検定にもWilcoxonの順位和検定(=Mann–Whitney U検定)を推しているが、等分散が前提となるのではないか。

下記資料においても等分散を求めていない。

https://www.heisei-u.ac.jp/ba/fukui/pdf/stattext12.pdf

Wikipediaによると

> U検定は2標本が共通の分布に基づくかどうかを検定するものであって、平均は同じだが分散は異なるような分布に基づく場合には偽の有意な結果が出ることもある（モンテカルロ法を用いて示されている）。

  マン・ホイットニーのU検定 - Wikipedia[^36]


やはり等分散か確認しほうが良さそう。これを確認されている方がいて、「マイナーだけど最強の統計的検定 Brunner-Munzel 検定」[^37]によると以下のように分けられる。

| 手法  | 正規性 |　等分散性 |
| :--- | :---: | :---: |
| Student’s t | 要 | 要 |
| Welch’s t   | 要 | 不要 |
| Mann-Whitney | 不要 | 要 |
| Brunner-Munzel | 不要 | 不要 |

　これだけだと正規性と等分散性を不要とするBrunner-Munzelが一番良さそうだがそれぞれ得手不得手があって、この方の検証だと下記と結論している。

結論
- 平均値の検定には Welchのt検定、中央値の検定には Brunner-Munzel検定
  - （Wilcoxonの順位和検定は特段の理由がなければ不要）

コード例
- Rのt.testならデフォルトでWelchですね。
- Pythonならscipyのstatsを用いて stats.ttest_ind(a, b, equal_var=False) とするようです。 

- Rならlawstatライブラリを用いて brunner.munzel.test(a, b)
- Pythonならscipyのstats.brunnermuzelを用いて brunnermunzel(a, b)

[^34]:https://twitter.com/h_okumura/status/1067353061310455808
[^35]:丹後俊郎． 新版 統計学のセンス． 新版， 朝倉書店， 2018， 176p.
[^36]:https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%B3%E3%83%BB%E3%83%9B%E3%82%A4%E3%83%83%E3%83%88%E3%83%8B%E3%83%BC%E3%81%AEU%E6%A4%9C%E5%AE%9A
[^37]:https://hoxo-m.hatenablog.com/entry/20150217/p1


## 8. 標準偏差・標準誤差、信頼区間・予測区間

- 標準偏差は母集団から得られたデータのばらつき。
- 標準誤差は母集団から抽出された標本から得られる推定量のばらつき。

難しいけれども母集団そのもののばらつきなのか、母集団から抽出した標本のばらつきなのかというところ。

c.f. 18-5. 標準偏差と標準誤差[^38]

> 95%信頼区間は真の値が95%の確率で含まれる推定区間

という誤用がある。正しくは母集団から100個のサンプルを抽出して信頼区間を100個作成するとそのうち95個の信頼区間は真の値を含む。ということ。
　
c.f. コラム：サンプル数再考／労働政策研究・研修機構（JILPT）[^39]
　
また、特に回帰のときに混同しやすいが、

> 95％信頼区間を表示したのにデータの95%がその区間に入っていない

　という誤用。この場合の信頼区間は、この試行を100回くらい返したら95回はこの信頼区間内に回帰直線が収まるということ。データが95%収まるのは予測区間の方でかなり広くなります。

c.f. おっと危ない：信頼区間と予測区間を混同しちゃダメ[^40]

[^38]:https://bellcurve.jp/statistics/course/8616.html
[^39]:https://www.jil.go.jp/column/bn/colum0151.html
[^40]:http://takehiko-i-hayashi.hatenablog.com/entry/20110204/1296773267


## 9. p値ハッキング

2016年にアメリカ統計協会が声明を出した。
　
> 　「統計的有意性と p値に関する ASA 声明」（日本計量生物学会による和訳）[^41]
> 1. P 値はデータと特定の統計モデル（訳注:仮説も統計モデルの要素のひとつ）が矛盾する程度をしめす指標のひとつである。
> 2. P 値は、調べている仮説が正しい確率や、データが偶然のみでえられた確率を測るものではない。
> 3. 科学的な結論や、ビジネス、政策における決定は、P値がある値（訳注: 有意水準）を超えたかどうかにのみ基づくべきではない。
> 4. 適正な推測のためには、すべてを報告する透明性が必要である。
> 5. P 値や統計的有意性は、効果の大きさや結果の重要性を意味しない。
> 6. P 値は、それだけでは統計モデルや仮説に関するエビデンスの、よい指標とはならない。
　（引用者注：通常は小文字で「p値」だが和訳原文ママ）

　うーむ。厳しい。
この中で4について「P値ハッキング（pハッキング）」という言葉が登場する。

『瀕死の統計学を救え！』豊田秀樹[^42]
> 学術的に有用でないのに、p<0.05を導いてしまう状態の総称です。神の見えざる手、ゾンビ問題、多重性問題によってpハッキングが生じます。

> ゾンビ問題
> 資源固定意図3やp値モニター意図4を心中に有していたのに、教科書的意図1を装って分析された検定は、見かけ上まったく区別がつかないという意味で（中略）以後、それらをゾンビ的検定（あるいはゾンビ）と呼びます。
>
> - 教科書的意図
>   - n人を固定して観察する
>   - 資源固定意図
>     - 研究期間を例えば1年と固定してその結果n人を観察した→本来、教科書的意図とは異なり教科書的意図の検定は成り立たない
>   - p値モニター意図
>     - 何度も何度も検定して、その内たった1回でもp値が0.05を下回ったら終了する追試実験

「p 値を使って学術論文を書くのは止めよう 」豊田(2017)[^43]
> - 神の見えざる手
>   - p 値が 5% を下回れ ば，統計的に有意な差があると認められ，その判定が重視される査読雑誌では，投稿される論文の データ数の分布を大きい方向にスライドさせる圧力が働き続ける。
>
>   - p<0.05になれば論文に載り、p>=0.05の結果が出れば載らないのであたかもp<0.05の研究結果ばかりが出るように思われる、ということだと思われる。
>
> - 多重性問題
>    - さらに問題なのは多重比較の方法が，本質的に多数存在することである。これは，分散分析が分散の比という 1 次元の検定統計量 F に対して棄却域を定めることができるのに対して，多重比較は注目すべき検定統計量の選択ばかりでなく，多次元の確率事象に対する棄却域の決め方は複雑だからである。このため多重比較による数種の有意性検定は，互いに一致しない判断を示し得るし，分散分析の判断とも一致しないことも少なくない。これは 1 次元の統計量を多次元の統計量に拡張することによって生じるデメリットである。理論が複雑で，何故その多重比較を使ったのかを説明することが難しいので，__つい沢山適用して有意になった多重比較で論文を書くことになりやすい。__
>    （引用者注：太字引用者）
>
> 極端に言えば5%未満を出そうと100回異なる検定を繰り返し、その中で5%未満となった結果のみを報告するということもあり得る。

 では、どうしたらいいのだろう。信頼区間などを示す、ベイズ的確率を用いる、などが提案されている。豊田先生はベイズ的確率を用いたphcという指標を提案している。

[^41]:https://www.biometrics.gr.jp/news/all/ASA.pdf
[^42]:豊田秀樹． 瀕死の統計学を救え！． 朝倉書店， 2020， 144p.
[^43]:https://www.jstage.jst.go.jp/article/sjpr/60/4/60_379/_pdf


## 10. 相関と因果-1

マーケティングに関してデータの扱いや実践的手法を解説している『マーケティング・エンジニアリング入門』[^44]に、相関と因果は違うという説明もないまま

> 因果関係を表現する代表的な数理モデルである回帰分析
> 回帰分析のように因子間の因果関係も同時に評価
　
　という記述がある。因果推論のための材料の一つとして回帰分析はあり得ると思いますが、この記述はあまりにもミスリード。

　アイスの売上と海難事故は相関しているのでアイス販売をなくせば海難事故は減るはず！

　という露骨な例だと相関と因果は違うとわかるのですが、実際の例となると入り組んでいて相関と因果を混同している例が多々あります。

　先の例の場合ですと「アイス売上増」と「海難事故増」の元の原因となる「夏、気温高」があり、これにより2つが増えているのであって、「アイス売上増」が「海難事故増」の原因ではなく、逆でもない。こういう「夏、気温高」の要因を交絡要因という。これがあるため、単純な相関分析では因果は分からない（なくてもアイスと海難事故のどちらが原因なのかは分からないですが）。

<img width="440" alt="スクリーンショット 2023-05-05 15 24 40" src="https://user-images.githubusercontent.com/17631286/236389853-688e3eef-23c9-46d6-bf9e-7d8e0df601e4.png">

　原因、結果、交絡要因の関係は上記以外にもあり複雑です。ご専門の国立環境研究所の林先生がまとめた記事「因果関係がないのに相関関係があらわれる４つのケースをまとめてみたよ（質問テンプレート付き）」[^45]が分かりやすいです。

> - 偶然によるケース
>   - p値が小さくてもたくさん検定したら相関があるように見えるものが出てきてしまうなど
> - 因果の流れが「逆」のケース
>   - 事故が多いから「交通事故多し」の看板が多いのに、「『交通事故多し』の看板が多いと事故が多いから看板を撤去しよう」というようなこと
> - 因果の上流側に共通の要因が存在するケース
>   - いわゆる交絡。
>   - 「テレビの台数が増えたので平均寿命が伸びた」などという例。経済が豊かになったなどの共通の要因があるのを無視して結果だけを比較しているケース。[^46]
> - 因果の合流点において選抜／層別／調整されてしまっているケース
>   - 大学に合格した生徒のみで、学力テストと実技テストの相関を見るなどのケース

これらを避けるための質問事項もあり、この記事は必読。

[^44]:上田雅夫、生田目崇． マーケティング・エンジニアリング入門． 有斐閣， 2017， 300p.
[^45]:http://takehiko-i-hayashi.hatenablog.com/entry/20130418/1366232166
[^46]:https://shirousagi.hatenablog.jp/entry/modamasarenaitamenokagakukogi/


## 11. 相関と因果-2

 理科の実験であれば概ねの場合、x以外の条件を全く同じにして、xを実施した、xを実施しない、の2群を比較するとxの影響でyが起こったのか起こらないのかを実験できる。a群のサンプルとb群のサンプルはxの実施有無以外は全く同じなので問題がない。
 ただ、ビジネス、社会、医学の場合では「x以外を全く同じにした2人/2群」を用意するのは事実上不可能である。

||ケース（措置群）|コントロール（正常群）
|:---|:---:|:---:|
|xを実施した|a群|（存在しない）|
|xを実施していない|（存在しない）|b群|

- 理科の実験の場合
  - a群とb群の比較に問題がない（問題がないようにx以外の条件を揃えることが出来る）。
- ビジネス、社会、医学の場合
  - a群とb群の比較に問題がある（問題がないようにx以外の条件を揃えることが困難か不可能）。
　
 ここでは後者、ビジネス、社会、医学の場合を考える。

- ランダム化比較試験
  - サンプルサイズ
- 回帰不連続実験
- マッチング
- 操作変数法
- 差の差推定・固定効果推定
- 合成コントロール法
- 回帰分析・単純比較
- 前後比較
c.f. 『政策評価のための因果関係の見つけ方』[^48]

などの手法がある。

[^47]:ここでの「理科の実験」とは薬品Aに試薬αを混ぜた反応の実験等であり、これであれば同じ薬品Aを2つ用意して温度や量を同じにして試薬αを混ぜたものと混ぜないものを比較できる、というような意味。台風Aが来たときの川の氾濫を防波堤施行αの有無で実験する的なことになると困難化不可能となる。
[^48]:エステル・デュフロ、レイチェル・グレナスター、マイケル・クレーマー． 政策評価のための因果関係の見つけ方． 日本評論社， 2019， 160p.


## 12. 多重共線性

 重回帰分析をした場合に、説明変数間で強い相関があること。かなり脅してくるWeb記事が多いのだがどれくらいで「強い」と見なされるのか、どのような不具合があるかについては記事によってふにゃふにゃ違う。


 多重共線性の指標としてはVIF値というものがあり、

```math
VIF=\frac{1}{1-r_{12}^2}
```
```math
r_{12}^2: 説明変数1と2の相関係数
```

 となっている。これについてWeb記事の場合、VIF>10で多重共線性を疑う、という記述が多いようですが、下記の2つ目の書籍ですとVIF>5で疑う必要があると言っているようです。ただ、この値だからダメでこの値だったらよいという基準値があるわけではなく、あくまでも怪しいのでよく見ろ、という大まかな目安があるという程度のものです。
 私の経験ですと、
- 説明変数がきっちり決まっていることは少なく、ざっくり当てはめそこから絞り込むような条件であればVIFが高いものをざっくり減らすか、Lassoなどの手法を用いる
- 変数Aと変数Bのどちらかがより一般的ならどちらかに絞る
- 変数Aと変数Bの原因となる変数αがあるのであればAとBではなくαと入れ替える

などの方が、色々と複雑な加工や解析を行うよりもまずは重要かと思います。

 定評のある書籍で調べてみました（すみません回帰分析自体の専門書は持っていないのですが）。

- 『計量経済学 (y21) 』[^49]
  - 私が知る中では最も詳しく、引用すると長いので要旨のみ。
    - 「多重共線性は悪者か？」という1節を設けて場合によっては望ましい結果となることを説明しています。
    > 説明変数を減らす、変数の変換、階差をとる、データ数を減らすなどの「民間療法」は副作用を伴い必ずしも正しいとは言えない
- 『Rによる実践的マーケティングリサーチと分析』[^50]
  > 一般的な経験則として、VIF>5.0であれば共線性を軽減する必要があります。
  > （中略）
  > 共線性を緩和する3つの一般的な戦略があります。（以下要旨）
  > 相関の強い変数を削除する
  > 主成分分析を実施する
  > 共線性に対して頑健なランダムフォレスト法等を用いる
- 『人文・社会科学の統計学』[^51]
  > 多重共線性は、実際に分析を行う際に十分に注意しなければならない。この問題が生じたときの対応は、方法として最も簡単には、説明変数をよく点検して原因となっている変数を取り除くか、あるいはそのままとして分析法を工夫するかである。後者としてはリッジ推定、もしくは主成分回帰などの方法が有効である。
- 『実証分析のための計量経済学』山本　中央経済社[^52]
  > （引用者注：説明変数1と2が）ほとんど似た動きをするため、多重共線性によって片方あるいは両方が有意でなくなることが予想されます。そうした場合、片方ずつを説明変数に用いた推定を行い、それぞれの結果を考察するなどの対処がとられます。
- 『実証分析入門』[^53]
  > 実際の実証分析においては多重共線性を気にする必要性は、必ずしも高くはない。自分が興味関心のある変数とは関係ないところで多重共線性が発生している場合には放置してかまわないし、自分が興味関心のある問題に多重共線性が発生している場合であっても、関連する変数を全て取り込むことが理論的に妥当だと確信できるような場合には、これを外すべきではない。

　結果としてはWeb記事が脅すほどではないし、むしろむやみに説明変数を減らすなというニュアンスが強い。10前後の確度の高い説明変数がある場合と、数十から数百の説明変数を絞り込む場合などでもまた違うのでしょうが。

結論
- 何のために分析しているのかに立ち戻る
- 重回帰分析以外が適切な場合もある
- それらを検討せずVIFで機械的に削除は必ずしも正しくない。削除するならば説明変数の意味を検討しながら

[^49]:浅野皙、中村二朗， 計量経済学 (y21) ． 第二版， 有斐閣， 2009， 343p.
[^50]:C.Chapman and E.McDonnell Feit． Rによる実践的マーケティングリサーチと分析． 原著第2版， 共立出版， 2020， 618p.
[^51]:東京大学教養学部統計学教室． 人文・社会科学の統計学． 東京大学出版会， 1994， 404p.
[^52]:山本勲． 実証分析のための計量経済学． 中央経済社， 2015， 160p.
[^53]:森田果． 実証分析入門． 日本評論社， 2014， 328p.


## 13. 見せかけの回帰

 よく政治経済の記事で、「〇〇と日経平均は連動している。回帰係数も高い」というような記事がありますが、ほとんどの場合それは見せかけの回帰です。

 馬場真哉 氏の記事「時系列データへの回帰分析」[^54]が詳しいですが
- 日経平均をはじめ経済金融の時系列データのほとんどは、前日の値に今日の乱数を足し合わせるというランダムウォークとなっている
- 単位根過程の代表的なものがランダムウォーク
- 単位根過程は見せかけの回帰を起こしやすいことが知られており、単純に相関を見てはだめとなります。ではどうしたらよいかというと
  - 前日の値との差分（階差）をとって比較する
  - 共和分になっているかどうか確認する

 いずれにしろ、時系列のグラフと相関係数を出して（それしか出さないで）「〇〇と△△は連動している。バツバツは有効な政策だ／無効な政策だ／陰謀だ／私は予測できる」などと言っている記事は疑ってかかったほうがよいと思います。
 
 より詳しくは、『経済・ファイナンスデータの計量時系列分析』[^55]

結論
- 経済金融等時期列データの相関等関係性を比較するときは気をつける。

コード例：R

<img width="265" alt="スクリーンショット 2023-05-05 15 47 28" src="https://user-images.githubusercontent.com/17631286/236393064-ec824de5-ac04-45ba-8cfa-f600c2fece6c.png">
一見、株価や為替のように意味ありげなグラフがある。x軸が300以降はまるで連動しているかのような動きをしている。

<img width="264" alt="スクリーンショット 2023-05-05 15 48 29" src="https://user-images.githubusercontent.com/17631286/236393307-7f7898ca-1559-47aa-92a0-5f9d5736b265.png">
先ほどのデータの散布図をみると弱いながら相関があり、右肩上がりとそうでない時と何らかの意味ありげなデータに見える。

<img width="272" alt="スクリーンショット 2023-05-05 15 48 44" src="https://user-images.githubusercontent.com/17631286/236393264-1ffec308-5033-48c4-9bfa-c30e048bb9f1.png">　
ところが、実はこの2つの変数は乱数で生成した無相関のデータ。乱数で生成した無相関のx, yをそれぞれ1つ前の値に次の値を加えるというランダムウォークの値（前の2図の変数xx, yy）にすると、あたかも意味ありげに相関があるように振る舞ってしまう。

 本当にこの2変数に相関があるか否かはランダムウォークとしてではなく、日々の値つまり経済指標等であれば階差をとった値で見ないといけない。


```R
set.seed(123456)
x<-rnorm(1000)
y<-rnorm(1000)
xx<-cumsum(x)
yy<-cumsum(y)
d<-data.frame(i = seq(1, 1000), x, y)

ggplot(d) + geom_line(aes(x = i, y = xx)) + geom_line(aes(x = i, y = yy), color = 'red') + labs(x = '', y = '')

qplot(xx, yy, main = sprintf('r=%.4f', cor(xx, yy)))

qplot(x, y, main = sprintf('r=%.4f', cor(x, y)))
```

[^54]:https://logics-of-blue.com/time-series-regression/
[^55]:沖本竜義． 経済・ファイナンスデータの計量時系列分析． 朝倉書店， 2010， 199p.


## 14. 欠損値・欠測値データはむやみに捨てるな

 欠損値や欠測値がある場合、
- その行や列ごと削除する
- 平均値や多数決で代入する
- NAとして一つのカテゴリとする

とすることが多いと思われる。

 前述の三中信宏氏によりますと

> 欠損値がある場合はそれを含む「行」と「列」を除去する場合が多いです．隣接するデータから補間することにより，欠損値を “修復” することはできないわけではありませんが，あくまでもそれは緊急措置です．[^56]

 機械学習ですと、『Kaggleで勝つデータ分析の技術』[^57]によりますと

>　xgboostやlightgbmといったGBDTのライブラリでは欠損値をそのまま扱うことができます。ですので、GBDTを使う場合には、欠損値をのまま扱うのが基本的な選択となります。
>　GBDTでないモデルの多くは欠損値を含む学習データを取り扱うことができず、（中略）欠損値を何らかの値で埋める必要があります。
> （中略）
> そのまま取り扱う
> 欠損値を代表値で埋める
> 欠損値を他の変数から予測する
> 欠損値から新たな特徴量を作成する
	

　ただ、欠測値の専門家によると、『欠測データ処理』[^58]
> 欠測は3つに分けられる。
  > 　MCAR	：完全に無作為な欠測であり、削除しても問題がない。
  > MAR	：条件付きで無作為な欠測。削除すると偏りの可能性がある。
  > NMAR	：無作為でない欠測。削除すると偏りの可能性がある。
> 不完全なデータを行、列、で削除してしまうと、MCARの場合は良いがそうでない場合は偏りの可能性がある。またデータの削除により検出力が下がる。
>　分析の目的が平均値や合計値の算出ならば単一の値を代入すればよいが、母集団パラメーターの推定ならば単一の代入では標準誤差を過小評価し、分析結果の精度を誇張してしまう。

 ということで、Kaggleのように目的変数の値を予測する場合以外では単純な削除、代入は避け、よく検討したほうがよいと思われます。

結論
- 欠損値（欠測値）が無作為なものなのかよく検討する
- 無作為でない場合には安易に削除することは悪影響があることを認識する
- 分析の目的や合計値の算出でない場合には平均値等の単一の代入では誤差を過小評価することを認識する
- 多重代入法を検討する

[^56]:https://twitter.com/leeswijzer/status/1062572767462244357
[^57]:門脇大輔、阪田隆司、保坂桂佑、平松雄司． Kaggleで勝つデータ分析の技術． 技術評論社， 2019， 424p.
[^58]:高橋将宜、渡辺美智子． 欠測データ処理． 共立出版， 2017， 192p.


## 15. 【要追記】過学習


## 16. 目的・目標・戦略・問題・課題
　
 データ分析と直接関係なさそうで、そんなの知ってるよと言われそうですが、曖昧に適当にしている人が多い。だいたい「目的・目標」、「問題・課題」などと並列に列挙している人は分かってない人が多い。目的と目標はどう違うのか、違うならその文章で言いたい方の言葉を書けばいいし、違いが分かってないなら言わない方がいい。
　
 まず、戦略と目的については、音部大輔氏の著書[^59]、記事「『戦略が重要』と口では言うけれど、実は本当に『戦略』を考えていることは少ない」[^60]も参考となる。

> __戦略とは「目的達成のための資源利用の指針」__

 つまり、資源が限られているのを意識せず「ぜんぶやろうぜ」「顧客回り徹底」などは戦略ではない、ということです。


 目的と目標の違いについては、小宮一慶氏の記事「成功するリーダーは、「目的」と「目標」の違いを理解している」[^61]が参考となる。

>	__そもそも「目的」とは何でしょうか。それは「存在意義」です。「目標」とは、その通過点であったり、目的達成の手段のことです。__

　
 つまり、「目的・目標は顧客第一」などというのは間違い。

　
 最後に、問題と課題の違いについては、多田翼氏の記事「『問題』 と『課題』の違いを言えますか？問題を正しく捉えるための手順」[^62]が分かりやすい。

>	__問題： ｢現状｣ が ｢あるべき姿｣ になっていない阻害要因__
> __課題：問題 (あるべき姿を妨げている要因) を解決するためにやること__
　
 つまり、システムが止まってしまうがログ分析が手作業で手間が掛かり復旧まで時間が掛かる
　　のであれば、
- 問題：ログ分析が手作業で手間が掛かる
- 課題：ログ分析のマニュアル化や自動化
　　
  であり、以下は誤り
- 問題：ログ分析のマニュアル化（←マニュアル化がいいのか自動化がいいのかそもそもの問題が何だったのか、マニュアルありきになりがち）
- 課題：ログ分析が手作業で時間が掛かる（←問題が分析できておらずどう解決すべきかまとまっていない）

 これらをきちんと設定せず、データ分析のPoC疲れとか言われても、それあなたビジネスじゃなくて趣味でやってよって感じ。

結論
-何を目指すのか目的を明確にする
-目的を達成する通過点として目標を置く
-目的達成のために限られた資源（ヒト・モノ・カネ）をどう有効活用するのかの指針として戦略を考える
-目的達成の阻害要因として問題がある
-問題を解決するためにやるべきことが課題

それがあって初めて、AIなりDXであり、そうじゃなきゃお遊びなので仕事以外でしてくださいな。

[^59]:音部大輔． なぜ「戦略」で差がつくのか。―戦略思考でマーケティングは強くなる―． 宣伝会議， 2017， 320p.
[^60]:https://www.advertimes.com/20170324/article246012/
[^61]:https://diamond.jp/articles/-/133511
[^62]:https://note.com/tsubasatada/n/naf4f8afa1b5d


#　II部：知っていると得
## 1. MIC vs HSIC：直線でなくとも相関はある⁉️
<img width="278" alt="スクリーンショット 2023-05-05 16 04 41" src="https://user-images.githubusercontent.com/17631286/236395857-8f68ec32-aaef-4627-a718-97f021ab5c05.png">
図のようなデータがあったとする。明らかな周期性がみられ、sin関数等でモデル化したいところ。ただ、データ分析では図を描くのが必須中の必須ですが、このような図が100, 1,000, 10,000枚枚となると見ていられない。相関係数などの指標でふるいにかけたいがこの図の場合はr=0.024となる。
 非線形だが図のような関係がある場合に高い値となる指標はないだろうか。というところで、

SlideShare 21世紀の手法対決 (MIC vs HSIC)[^63]

 これは2013年のR勉強会@東京のスライドだが、教科書等で見たことなく、ネットの記事も少なめなのでご紹介。この図で計算をすると、MIC=0.938[^64]。相関係数と同じく1に近いと相関が高い。HSICは独立性の検定として用いられこの図だとp値=0.000999[^65]。良し悪しや、改善された指標も研究されている。詳細は前述だが、たくさんのデータから関係性を見つけたいときに知ってると知らないで大きく違う。

結論
- （ピアソン、スピアマン、ケンドールの）相関係数と同じ感覚でHICやHSICを使う時代へ

コード例:R
```R
library(ggplot2)

set.seed(12345)
x = seq(-1, 1, 0.002)
y = sin(x / 0.4 * 3.14159) + rnorm(1001, sd = 0.2)
qplot(x, y, main = sprintf('r=%.4f', cor(x, y)))

library(minerva)
mine(x, y)

library(dHSIC)
dhsic.test(x, y)
```

[^63]:https://www.slideshare.net/motivic/tokyo-r-lt-25759212
[^64]:R/minerva::mineで計算
[^65]:R/dHSIC::dhsic.testで計算


## 2. 「データが足りないなら増やせば良いじゃない」
 「データが足りないなら増やせば良いじゃない。」[^66]とマリー・アントワネットが言いそうな無茶なセリフ[^67]だけれど、実際に増やせるし、増やすのが王道な分析もある。
 深層学習というとなんだかすごそうだが、画像が横向きや回転や反転、ノイズが入っていると認識率が落ちたりまったく認識できなかったりする。ならば横向きや回転や反転、ノイズを入れて全て学習してしまえばいいというのがData Augumentationである。「【基本編】画像認識に使用されるData Augmentationを一挙にまとめてみた！」[^68]が詳しい。

 人間の目からしたら横向きや回転や反転、ノイズ等があっても元の画像と一緒なので上記のData Augumentationは至極妥当で腑に落ちる。ではそうではないいわゆるテーブルデータでも増やせるのだろうか。

 元々はデータを増やすということではなく、判別問題での不均衡データつまり、

|Positive|Negative|
|---:    |---:    |
|9,950   |      50|

 このデータで学習そしてもNegativeの学習サンプルが少なく、全てをPositiveと予測するようなことになりやすい。

|        |予測Positive|予測Negative|
|:---:   |       ---:|        ---:|
|Positive|      9,950|           0|
|Negative|         50|           0|

- 感度=TP/(TP+FP)=99.5%
- 特異度=TN/(FN+TN)=NaN

 このような不均衡なデータの場合にはPositiveを減らすDown SamplingとNegativeを増やすOver Samplingがある。
 Down Samplingはランダムに値を取ってくるだけなので簡単だが、学習データが減ってしまうという難点がある。Over Samplingはないデータを生成するということになるがそこにSMOTEという技術が発表された。
 「解説編：オーバーサンプリング手法解説 (SMOTE, ADASYN, Borderline-SMOTE, Safe-level SMOTE)」[^69]、「【ML Tech RPT. 】第4回 不均衡データ学習 (Learning from Imbalanced Data) を学ぶ(1)」[^70]、書籍では『前処理大全』[^71]が詳しい。
 いろいろな変法があるが大元は、K-近傍法を用いて近しいデータから内挿して増やすとのこと。回帰版のSMOTE Regressionもある。一瞬これでいいのか？と思いますが、「SMOTEは効かないことが多い」[^72]との記事もあり。自分のデータで検証する必要がある。

[^66]:https://qiita.com/cvusk/items/aa628e84e72cdf0a6e77
[^67]:実はマリー・アントワネットの言葉ではないらしい https://ja.wikipedia.org/wiki/%E3%82%B1%E3%83%BC%E3%82%AD%E3%82%92%E9%A3%9F%E3%81%B9%E3%82%8C%E3%81%B0%E3%81%84%E3%81%84%E3%81%98%E3%82%83%E3%81%AA%E3%81%84
[^68]:https://ai-scholar.tech/articles/data-augmentation/data-augmentation-image-recognition-survey
[^69]:https://qiita.com/eigs/items/8ae0970afe188a1124d1
[^70]:https://buildersbox.corp-sansan.com/entry/2019/03/05/110000
[^71]:本橋智光． 前処理大全． 技術評論社， 2018， 321p.
[^72]:「ML_BearのKaggleな日常」https://naotaka1128.hatenadiary.jp/entry/kaggle-compe-tips


## 3. Permutation Importance：機械学習の中身の解釈のしかた1
 木系のアルゴリズムだとFeature Importanceなどの手法で特徴量（説明変数）がどれだけ予測に効いているかという重要度を示すことが出来たが、どんなアルゴリズムにも適用できるという手法はなかった。
　「Permutation Importanceを使ってモデルがどの特徴量から学習したかを定量化する」[^73]の記事が詳しいが、Permutation Importanceという手法が出来た。仕組みは単純で
- 学習して予測モデルを構築する
- 特徴量ごとに下記を計算
  - 着目している特徴量をランダムに並び替えて予測精度の変化を見る
  - その特徴量が予測に強い影響を持っているならば予測精度が大幅に下がる
  - その特徴量が予測にあまり影響をしていないならば予測精度はあまり下がらない

Pythonのライブラリ[^74]もRのライブラリ[^75]もある。

[^73]:https://www.datarobot.com/jp/blog/permutation-importance/
[^74]:https://scikit-learn.org/stable/modules/permutation_importance.html
[^75]:https://rdrr.io/cran/mmpf/man/permutationImportance.html


## 4. 部分依存度プロット：機械学習の中身の解釈のしかた2
 部分従属プロットとも言うらしい。
 説明変数x1が1増えたら、1減ったら目的変数がどう変化するか。重回帰分析でないとどうしょうもない感じですが、うまいこと考えたもんだなって感じ。
 何らかのアルゴリズム（ということで逆にいうとどんなアルゴリズムでも対応可）で学習して予測モデルを作成する。そして他の説明変数を固定して、説明変数x1を100としたら、200としたら、……、どうなるかを計算していく。結果として説明変数x1が1変化したら目的変数がどう変化するかがわかるというもの。
　
|目的変数y|説明変数x1|説明変数x2|説明変数x3|......|
|---:    |---:    |---:     |---:     |---:     |
|......  |......  |......  |......  |......  |
|85      |123     |2.3     |True    |......  |
|97      |519      |4.9    |False   |......   |
|254     |1215    |1.1     |True    |......   |

　
 まず学習して予測モデルを作成
$y=f(x_i)$

 説明変数x1=100の場合

|目的変数y|説明変数x1|説明変数x2|説明変数x3|......|
|---:    |---:    |---:     |---:     |---:     |
|......  |......  |......  |......  |......  |
|80|100|2.3|True|......|
|81|100|4.9|False|......|
|156|100|1.1|True|......|


 説明変数x1=200の場合
|目的変数y|説明変数x1|説明変数x2|説明変数x3|......|
|---:    |---:    |---:     |---:     |---:     |
|......  |......  |......  |......  |......  |
|84|200|2.3|True|......|
|87|200|4.9|False|......|
|193|200|1.1|True|......|

 とシミュレーション的に計算していく。

c.f. 特徴量ごとの作用を使ってモデルの中身を解釈する[^76]

 Pythonでの使い方[^77]、Rでの使い方[^78]。
 
[^76]:https://www.datarobot.com/jp/blog/2018-02-15-modelxray/
[^77]:https://ushitora.net/archives/605
[^78]:https://qiita.com/nakamichi/items/bed7a2f180ea9ce86d94


## 5. SHAP：機械学習の中身の解釈のしかた3
 重回帰やロジスティック回帰、決定木や回帰木は目的変数に説明変数がどう効いているかが分かりやすい分析だが他の機械学習はブラックボックスだ。みたいな説明が今までは一般的で説明変数の影響度(Variable Importance)くらいの情報付加はあったのだけど、どの説明変数がこう変わると目的変数がどう変わるみたいなことはなかなか説明が出来なかった。
 ところが2017年のNIPSで発表された論文[^79]でSHAPという手法が開発された（SHAPも変法がたくさんあり、LIME等その他の指標も提案されている。
 参考になるのはたとえば「【記事更新】私のブックマーク『機械学習における解釈性（Interpretability in Machine Learning）』」[^80]）なんか難しそうで食わず嫌いしていたのですがあちこちで有名になったので調べてみたら原理は簡単で協力なので紹介したい。参考としたのは「可視化アルゴリズムSHAPを理解するために、ゲーム理論を調べてまとめました。」[^81]「SHAP(SHapley Additive exPlanations)で機械学習モデルを解釈する」[^82]の記事など。
 まずいきなりゲーム理論が出てきて面食らいます。「SHAP(SHapley Additive exPlanations)で機械学習モデルを解釈する」の記事ですと『ゲーム理論 新版』岡田章[^83]が引用されてますがこれはかなり本格的な本なので入門にはその弟分である『ゲーム理論・入門 新版--人間社会の理解のために (有斐閣アルマ)』岡田章[^84]のほうが分かりやすいです。
 集団の提携、競争により、個々人の成果により報酬を与えるときの基準となるシャープレイ値というものをまず考えます。

 シャープレイ値（後者『ゲーム理論・入門 新版--人間社会の理解のために (有斐閣アルマ)』の例より）。

- 問題設定
  - A, B, Cの3人が起業する
    - それぞれ単独、あるいは共同で起業した場合の企業価値への貢献度を下記と仮定する
      - Aのみ	60万円
      - Bのみ	40万円
      - Cのみ	20万円
      - AとB	200万円
      - AとC	150万円
      - BとC	100万円
      - A, B, C	240万円
    （複数で起業した場合は個人の努力の足し算以上の効果があると仮定）
- 限界貢献度
  - A, B, Cが1人追加した場合の企業価値向上への貢献度を限界貢献度と呼びそれを求める
  - 例
    - Aが起業しAの限界貢献度は60万円
    - Bが追加で参加すると200-40でBの限界貢献度は160万円
    - Cが更に追加で参加すると240-200で限界貢献度は40万円
    - A, B, Cの順番以外にも追加の順番があるのですべてを求めて平均を取る

||A|B|C|
|:---:|:---:|:---:|:---:|
|A B C|60|140|40|
|A C B|60|90|90|
|B A C|160|40|40|
|B C A|140|40|60|
|C A B|130|90|20|
|C B A|140|80|20|
|平均値|115|80|45|

- この平均値がA, B, Cそれぞれのシャープレイ値となる。
- このシャープレイ値を合計すると3人で起業したときの企業価値240万円に一致し、この配分でA, B, Cに報酬を与えるのが合理的であるとの考え方。

 このシャープレイ値を機械学習に応用する。企業価値を目的変数、シャープレイ値を説明変数の目的変数に対する影響度と考えると同じ考え方で、各説明変数が目的変数にどのていど影響しているかが分かる。
　「SHAP(SHapley Additive exPlanations)で機械学習モデルを解釈する」の記事が特に分かりやすいかと思いますが、起業の例と同様に、
- 説明変数として $X=X_1, X_2, X_3$ の3つがある。目的変数はf(X)
- 説明変数が何もないと目的変数はE[f(X)]となる
- $X_1=x_1$ を付け加えると $X_1$ のシャープレイ値を加えて目的変数が $E[f(x)|X_1=x_1]$ となる
- 同様に $X_2=x_2$ を付け加えると $X_2$ のシャープレイ値を加えて $E[f(x)|X_1=x_1, X_2=x_2]$ となり
- 同じく同様に $X_3=x_3$ を付け加えると $X_3$ のシャープレイ値を加えて $E[f(x)|X_1=x_1, X_2=x_2, X_3=x_3]$ となる（シャープレイ値はマイナスもありうる）
<img width="581" alt="スクリーンショット 2023-05-05 16 39 28" src="https://user-images.githubusercontent.com/17631286/236401964-3f4d2ece-27f2-44dc-96a3-45cfcbcc389d.png">


 Pythonでの使い方[^85]、Rでの使い方[^86]。

[^79]:http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf
[^80]:https://www.ai-gakkai.or.jp/my-bookmark_vol33-no3/
[^81]:https://yhiss.hatenablog.com/entry/2019/09/21/160621
[^82]:https://dropout009.hatenablog.com/entry/2019/11/20/091450
[^83]:岡田章． ゲーム理論 新版． 有斐閣， 2011， 496p.
[^84]:岡田章． ゲーム理論・入門 新版--人間社会の理解のために． 有斐閣， 201４， 334p.
[^85]:https://github.com/slundberg/shap
[^86]:https://cran.r-project.org/web/packages/shapper/vignettes/shapper_regression.html
